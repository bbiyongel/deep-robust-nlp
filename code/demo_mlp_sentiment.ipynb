{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment classification with MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages loaded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from util import load_directory_data,load_dataset,download_and_load_datasets,gpusession,random_flip_target\n",
    "from mlp_cls_class import mlp_cls_class\n",
    "print (\"Packages loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'movie_review'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[data/movie_review.npz] Loaded. Size is [26.4009]MB\n"
     ]
    }
   ],
   "source": [
    "savename = 'data/' + dataset_name + '.npz'\n",
    "FORCE_RELOAD = 0 \n",
    "if os.path.isfile(savename) & (~FORCE_RELOAD): # load if the file exists,\n",
    "    # Load \n",
    "    l = np.load(savename)    \n",
    "    x_train = l['x_train']\n",
    "    x_test = l['x_test']\n",
    "    t_train = l['t_train']\n",
    "    t_test = l['t_test']\n",
    "    print (\"[%s] Loaded. Size is [%.4f]MB\" % (savename,os.path.getsize(savename)/1000./1000.))\n",
    "else: # otherwise, make the data\n",
    "    # Download dataset\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR) # Reduce logging output.\n",
    "    train_df, test_df = download_and_load_datasets()\n",
    "    # Check dataset\n",
    "    for i in range(2):\n",
    "        print (i,':',train_df['sentence'][i],'\\n')\n",
    "    for i in range(2):\n",
    "        print (i,':',train_df['polarity'][i])\n",
    "    embed_module = hub.Module(\"https://tfhub.dev/google/nnlm-en-dim128/1\")\n",
    "    embed_train = embed_module(tf.reshape(train_df[\"sentence\"], shape=[-1]))\n",
    "    embed_test = embed_module(tf.reshape(test_df[\"sentence\"], shape=[-1]))\n",
    "    with tf.train.MonitoredTrainingSession(is_chief=True) as sess:\n",
    "        x_train = sess.run(embed_train)\n",
    "        x_test = sess.run(embed_test)\n",
    "    n_train,n_test = np.shape(x_train)[0],np.shape(x_test)[0]\n",
    "    t_train,t_test = np.zeros((n_train,2)),np.zeros((n_test,2))\n",
    "    for i in range(n_train):\n",
    "        t_train[i,train_df['polarity'][i]] = 1\n",
    "    for i in range(n_test):\n",
    "        t_test[i,test_df['polarity'][i]] = 1    \n",
    "    print(\"Shapes of 'x_train' and 'x_test' are %s and %s.\"%\n",
    "          (x_train.shape,x_test.shape)) # (result: (1, 128))    \n",
    "    print(\"Shapes of 't_train' and 't_test' are %s and %s.\"%\n",
    "          (t_train.shape,t_test.shape)) # (result: (1, 128))    \n",
    "    # Save \n",
    "    np.savez(savename,x_train=x_train,x_test=x_test,t_train=t_train,t_test=t_test)\n",
    "    print (\"[%s] Saved. Size is [%.4f]MB\" % (savename,os.path.getsize(savename)/1000./1000.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add perturbation to t_label\n",
    "t_train_shuffle = random_flip_target(t_train,_rate=0.3,_seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Global Variables ====\n",
      " [00] Name:[mlp_cls_sentiment/lin0/weights:0] Shape:[[128, 128]]\n",
      " [01] Name:[mlp_cls_sentiment/lin0/BatchNorm/beta:0] Shape:[[128]]\n",
      " [02] Name:[mlp_cls_sentiment/lin0/BatchNorm/moving_mean:0] Shape:[[128]]\n",
      " [03] Name:[mlp_cls_sentiment/lin0/BatchNorm/moving_variance:0] Shape:[[128]]\n",
      " [04] Name:[mlp_cls_sentiment/lin1/weights:0] Shape:[[128, 128]]\n",
      " [05] Name:[mlp_cls_sentiment/lin1/BatchNorm/beta:0] Shape:[[128]]\n",
      " [06] Name:[mlp_cls_sentiment/lin1/BatchNorm/moving_mean:0] Shape:[[128]]\n",
      " [07] Name:[mlp_cls_sentiment/lin1/BatchNorm/moving_variance:0] Shape:[[128]]\n",
      " [08] Name:[mlp_cls_sentiment/out/weights:0] Shape:[[128, 2]]\n",
      " [09] Name:[mlp_cls_sentiment/out/biases:0] Shape:[[2]]\n",
      " [10] Name:[mlp_cls_sentiment/lin0/weights/Adam:0] Shape:[[128, 128]]\n",
      " [11] Name:[mlp_cls_sentiment/lin0/weights/Adam_1:0] Shape:[[128, 128]]\n",
      " [12] Name:[mlp_cls_sentiment/lin0/BatchNorm/beta/Adam:0] Shape:[[128]]\n",
      " [13] Name:[mlp_cls_sentiment/lin0/BatchNorm/beta/Adam_1:0] Shape:[[128]]\n",
      " [14] Name:[mlp_cls_sentiment/lin1/weights/Adam:0] Shape:[[128, 128]]\n",
      " [15] Name:[mlp_cls_sentiment/lin1/weights/Adam_1:0] Shape:[[128, 128]]\n",
      " [16] Name:[mlp_cls_sentiment/lin1/BatchNorm/beta/Adam:0] Shape:[[128]]\n",
      " [17] Name:[mlp_cls_sentiment/lin1/BatchNorm/beta/Adam_1:0] Shape:[[128]]\n",
      " [18] Name:[mlp_cls_sentiment/out/weights/Adam:0] Shape:[[128, 2]]\n",
      " [19] Name:[mlp_cls_sentiment/out/weights/Adam_1:0] Shape:[[128, 2]]\n",
      " [20] Name:[mlp_cls_sentiment/out/biases/Adam:0] Shape:[[2]]\n",
      " [21] Name:[mlp_cls_sentiment/out/biases/Adam_1:0] Shape:[[2]]\n",
      "Text name: res/res_mlp_cls_sentiment.txt\n",
      "[00/20] [Loss] train:0.655 test:0.581 [Accr] train:62.2% test:77.0% maxTest:77.0%\n",
      "[02/20] [Loss] train:0.648 test:0.569 [Accr] train:62.5% test:73.8% maxTest:77.0%\n",
      "[04/20] [Loss] train:0.623 test:0.559 [Accr] train:65.8% test:75.1% maxTest:77.0%\n",
      "[06/20] [Loss] train:0.599 test:0.557 [Accr] train:68.1% test:74.1% maxTest:77.0%\n",
      "[08/20] [Loss] train:0.575 test:0.572 [Accr] train:70.3% test:71.1% maxTest:77.0%\n",
      "[10/20] [Loss] train:0.548 test:0.609 [Accr] train:73.1% test:66.9% maxTest:77.0%\n",
      "[12/20] [Loss] train:0.530 test:0.626 [Accr] train:73.7% test:65.6% maxTest:77.0%\n",
      "[14/20] [Loss] train:0.486 test:0.616 [Accr] train:77.3% test:66.8% maxTest:77.0%\n",
      "[16/20] [Loss] train:0.463 test:0.649 [Accr] train:79.0% test:64.8% maxTest:77.0%\n",
      "[18/20] [Loss] train:0.448 test:0.687 [Accr] train:80.9% test:61.9% maxTest:77.0%\n",
      "[20/20] [Loss] train:0.432 test:0.666 [Accr] train:81.2% test:63.9% maxTest:77.0%\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph(); tf.set_random_seed(0)\n",
    "CNN = mlp_cls_class(_name='mlp_cls_sentiment',_x_dim=128,_t_dim=2,_h_dims=[128,128],\n",
    "                _actv=tf.nn.relu,_bn=slim.batch_norm,_l2_reg_coef=1e-6,_momentum=0.5,\n",
    "                _USE_SGD=False,_USE_MIXUP=False,_GPU_ID=0,_VERBOSE=True)\n",
    "sess = gpusession();sess.run(tf.global_variables_initializer()) \n",
    "CNN.train(sess,_x_train=x_train,_t_train=t_train_shuffle,_x_test=x_test,_t_test=t_test,\n",
    "              _max_epoch=20,_batch_size=128,_lr=1e-4,_kp=0.9,\n",
    "              _LR_SCHEDULE=0,_PRINT_EVERY=10,_VERBOSE_TRAIN=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
