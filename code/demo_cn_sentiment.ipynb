{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment classification with ChoiceNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages loaded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from util import load_directory_data,load_dataset,download_and_load_datasets,gpusession,random_flip_target\n",
    "from cn_cls_class import cn_cls_class\n",
    "print (\"Packages loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'movie_review'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[data/movie_review.npz] Loaded. Size is [26.4009]MB\n"
     ]
    }
   ],
   "source": [
    "savename = 'data/' + dataset_name + '.npz'\n",
    "FORCE_RELOAD = 0 \n",
    "if os.path.isfile(savename) & (~FORCE_RELOAD): # load if the file exists,\n",
    "    # Load \n",
    "    l = np.load(savename)    \n",
    "    x_train = l['x_train']\n",
    "    x_test = l['x_test']\n",
    "    t_train = l['t_train']\n",
    "    t_test = l['t_test']\n",
    "    print (\"[%s] Loaded. Size is [%.4f]MB\" % (savename,os.path.getsize(savename)/1000./1000.))\n",
    "else: # otherwise, make the data\n",
    "    # Download dataset\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR) # Reduce logging output.\n",
    "    train_df, test_df = download_and_load_datasets()\n",
    "    # Check dataset\n",
    "    for i in range(2):\n",
    "        print (i,':',train_df['sentence'][i],'\\n')\n",
    "    for i in range(2):\n",
    "        print (i,':',train_df['polarity'][i])\n",
    "    embed_module = hub.Module(\"https://tfhub.dev/google/nnlm-en-dim128/1\")\n",
    "    embed_train = embed_module(tf.reshape(train_df[\"sentence\"], shape=[-1]))\n",
    "    embed_test = embed_module(tf.reshape(test_df[\"sentence\"], shape=[-1]))\n",
    "    with tf.train.MonitoredTrainingSession(is_chief=True) as sess:\n",
    "        x_train = sess.run(embed_train)\n",
    "        x_test = sess.run(embed_test)\n",
    "    n_train,n_test = np.shape(x_train)[0],np.shape(x_test)[0]\n",
    "    t_train,t_test = np.zeros((n_train,2)),np.zeros((n_test,2))\n",
    "    for i in range(n_train):\n",
    "        t_train[i,train_df['polarity'][i]] = 1\n",
    "    for i in range(n_test):\n",
    "        t_test[i,test_df['polarity'][i]] = 1    \n",
    "    print(\"Shapes of 'x_train' and 'x_test' are %s and %s.\"%\n",
    "          (x_train.shape,x_test.shape)) # (result: (1, 128))    \n",
    "    print(\"Shapes of 't_train' and 't_test' are %s and %s.\"%\n",
    "          (t_train.shape,t_test.shape)) # (result: (1, 128))    \n",
    "    # Save \n",
    "    np.savez(savename,x_train=x_train,x_test=x_test,t_train=t_train,t_test=t_test)\n",
    "    print (\"[%s] Saved. Size is [%.4f]MB\" % (savename,os.path.getsize(savename)/1000./1000.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add perturbation to t_label\n",
    "t_train_shuffle = random_flip_target(t_train,_rate=0.3,_seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Global Variables ====\n",
      " [00] Name:[cn_cls_sentiment/lin0/weights:0] Shape:[[128, 128]]\n",
      " [01] Name:[cn_cls_sentiment/lin0/BatchNorm/beta:0] Shape:[[128]]\n",
      " [02] Name:[cn_cls_sentiment/lin0/BatchNorm/moving_mean:0] Shape:[[128]]\n",
      " [03] Name:[cn_cls_sentiment/lin0/BatchNorm/moving_variance:0] Shape:[[128]]\n",
      " [04] Name:[cn_cls_sentiment/lin1/weights:0] Shape:[[128, 128]]\n",
      " [05] Name:[cn_cls_sentiment/lin1/BatchNorm/beta:0] Shape:[[128]]\n",
      " [06] Name:[cn_cls_sentiment/lin1/BatchNorm/moving_mean:0] Shape:[[128]]\n",
      " [07] Name:[cn_cls_sentiment/lin1/BatchNorm/moving_variance:0] Shape:[[128]]\n",
      " [08] Name:[cn_cls_sentiment/rho_raw/weights:0] Shape:[[128, 5]]\n",
      " [09] Name:[cn_cls_sentiment/rho_raw/BatchNorm/beta:0] Shape:[[5]]\n",
      " [10] Name:[cn_cls_sentiment/rho_raw/BatchNorm/moving_mean:0] Shape:[[5]]\n",
      " [11] Name:[cn_cls_sentiment/rho_raw/BatchNorm/moving_variance:0] Shape:[[5]]\n",
      " [12] Name:[cn_cls_sentiment/muW:0] Shape:[[128, 2]]\n",
      " [13] Name:[cn_cls_sentiment/logSigmaW:0] Shape:[[128, 2]]\n",
      " [14] Name:[cn_cls_sentiment/var_raw/weights:0] Shape:[[128, 2]]\n",
      " [15] Name:[cn_cls_sentiment/var_raw/BatchNorm/beta:0] Shape:[[2]]\n",
      " [16] Name:[cn_cls_sentiment/var_raw/BatchNorm/moving_mean:0] Shape:[[2]]\n",
      " [17] Name:[cn_cls_sentiment/var_raw/BatchNorm/moving_variance:0] Shape:[[2]]\n",
      " [18] Name:[cn_cls_sentiment/pi_logits/weights:0] Shape:[[128, 5]]\n",
      " [19] Name:[cn_cls_sentiment/pi_logits/BatchNorm/beta:0] Shape:[[5]]\n",
      " [20] Name:[cn_cls_sentiment/pi_logits/BatchNorm/moving_mean:0] Shape:[[5]]\n",
      " [21] Name:[cn_cls_sentiment/pi_logits/BatchNorm/moving_variance:0] Shape:[[5]]\n",
      " [22] Name:[cn_cls_sentiment/lin0/weights/Adam:0] Shape:[[128, 128]]\n",
      " [23] Name:[cn_cls_sentiment/lin0/weights/Adam_1:0] Shape:[[128, 128]]\n",
      " [24] Name:[cn_cls_sentiment/lin0/BatchNorm/beta/Adam:0] Shape:[[128]]\n",
      " [25] Name:[cn_cls_sentiment/lin0/BatchNorm/beta/Adam_1:0] Shape:[[128]]\n",
      " [26] Name:[cn_cls_sentiment/lin1/weights/Adam:0] Shape:[[128, 128]]\n",
      " [27] Name:[cn_cls_sentiment/lin1/weights/Adam_1:0] Shape:[[128, 128]]\n",
      " [28] Name:[cn_cls_sentiment/lin1/BatchNorm/beta/Adam:0] Shape:[[128]]\n",
      " [29] Name:[cn_cls_sentiment/lin1/BatchNorm/beta/Adam_1:0] Shape:[[128]]\n",
      " [30] Name:[cn_cls_sentiment/rho_raw/weights/Adam:0] Shape:[[128, 5]]\n",
      " [31] Name:[cn_cls_sentiment/rho_raw/weights/Adam_1:0] Shape:[[128, 5]]\n",
      " [32] Name:[cn_cls_sentiment/rho_raw/BatchNorm/beta/Adam:0] Shape:[[5]]\n",
      " [33] Name:[cn_cls_sentiment/rho_raw/BatchNorm/beta/Adam_1:0] Shape:[[5]]\n",
      " [34] Name:[cn_cls_sentiment/muW/Adam:0] Shape:[[128, 2]]\n",
      " [35] Name:[cn_cls_sentiment/muW/Adam_1:0] Shape:[[128, 2]]\n",
      " [36] Name:[cn_cls_sentiment/logSigmaW/Adam:0] Shape:[[128, 2]]\n",
      " [37] Name:[cn_cls_sentiment/logSigmaW/Adam_1:0] Shape:[[128, 2]]\n",
      " [38] Name:[cn_cls_sentiment/var_raw/weights/Adam:0] Shape:[[128, 2]]\n",
      " [39] Name:[cn_cls_sentiment/var_raw/weights/Adam_1:0] Shape:[[128, 2]]\n",
      " [40] Name:[cn_cls_sentiment/var_raw/BatchNorm/beta/Adam:0] Shape:[[2]]\n",
      " [41] Name:[cn_cls_sentiment/var_raw/BatchNorm/beta/Adam_1:0] Shape:[[2]]\n",
      " [42] Name:[cn_cls_sentiment/pi_logits/weights/Adam:0] Shape:[[128, 5]]\n",
      " [43] Name:[cn_cls_sentiment/pi_logits/weights/Adam_1:0] Shape:[[128, 5]]\n",
      " [44] Name:[cn_cls_sentiment/pi_logits/BatchNorm/beta/Adam:0] Shape:[[5]]\n",
      " [45] Name:[cn_cls_sentiment/pi_logits/BatchNorm/beta/Adam_1:0] Shape:[[5]]\n",
      "Text name: res/res_cn_cls_sentiment.txt\n",
      "[00/20] [Loss] train:0.041 test:0.026 [Accr] train:55.1% test:62.6% maxTest:62.6%\n",
      "[02/20] [Loss] train:0.006 test:-0.027 [Accr] train:58.8% test:70.4% maxTest:70.4%\n",
      "[04/20] [Loss] train:-0.025 test:-0.070 [Accr] train:60.1% test:73.3% maxTest:73.3%\n",
      "[06/20] [Loss] train:-0.048 test:-0.097 [Accr] train:60.9% test:74.7% maxTest:74.7%\n",
      "[08/20] [Loss] train:-0.076 test:-0.124 [Accr] train:61.5% test:75.4% maxTest:75.4%\n",
      "[10/20] [Loss] train:-0.101 test:-0.151 [Accr] train:61.7% test:75.7% maxTest:75.7%\n",
      "[12/20] [Loss] train:-0.120 test:-0.167 [Accr] train:61.9% test:76.4% maxTest:76.4%\n",
      "[14/20] [Loss] train:-0.140 test:-0.187 [Accr] train:62.0% test:76.6% maxTest:76.6%\n",
      "[16/20] [Loss] train:-0.158 test:-0.205 [Accr] train:62.4% test:76.9% maxTest:76.9%\n",
      "[18/20] [Loss] train:-0.176 test:-0.224 [Accr] train:62.4% test:77.0% maxTest:77.0%\n",
      "[20/20] [Loss] train:-0.198 test:-0.247 [Accr] train:62.4% test:77.2% maxTest:77.2%\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph(); tf.set_random_seed(0)\n",
    "CN = cn_cls_class(_name='cn_cls_sentiment',_x_dim=128,_t_dim=2,_h_dims=[128,128],\n",
    "                  _k_mix=5,_rho_ref_train=0.95,_tau_inv=1e-4,_pi1_bias=0.0,\n",
    "                  _log_sigma_z_val=-2,_logsumexp_coef=0.1,_kl_reg_coef=0.1,\n",
    "                  _actv=tf.nn.relu,_bn=slim.batch_norm,_l2_reg_coef=1e-6,_momentum=0.5,\n",
    "                  _USE_SGD=False,_USE_MIXUP=False,_GPU_ID=0,_VERBOSE=True)\n",
    "sess = gpusession();sess.run(tf.global_variables_initializer()) \n",
    "CN.train(sess,_x_train=x_train,_t_train=t_train_shuffle,_x_test=x_test,_t_test=t_test,\n",
    "              _max_epoch=20,_batch_size=128,_lr=1e-5,_kp=0.9,\n",
    "              _LR_SCHEDULE=0,_PRINT_EVERY=10,_VERBOSE_TRAIN=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
